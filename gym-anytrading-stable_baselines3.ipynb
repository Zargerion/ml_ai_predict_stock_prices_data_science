{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_anytrading\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make('stocks-v0', df=fu_df, frame_bound=(5,120), window_size=5 )\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    n_state, rewards, done, info = env.step(action)\n",
    "    if done:\n",
    "        print('info', info)\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.cla()\n",
    "env.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_anytrading.envs import StocksEnv\n",
    "from finta import TA\n",
    "\n",
    "#fu_df['Volume'] = fu_df['Volume'].apply(lambda x: float(x.replace(\".\", \"\")))\n",
    "fu_df['SMA'] = TA.SMA(fu_df, 12)\n",
    "fu_df['RSI'] = TA.RSI(fu_df)\n",
    "fu_df['OBV'] = TA.OBV(fu_df)\n",
    "fu_df.fillna(0,inplace=True)\n",
    "fu_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_signals(env):\n",
    "    start = env.frame_bound[0] - env.window_size\n",
    "    end = env.frame_bound[1]\n",
    "    prices = env.df.loc[:, 'Low'].to_numpy()[start:end]\n",
    "    signal_features = env.df.loc[:, ['Low', 'Volume', 'SMA', 'RSI', 'OBV']].to_numpy()[start:end]\n",
    "    return prices, signal_features\n",
    "\n",
    "class MyCustomEnv(StocksEnv):\n",
    "    _process_data = add_signals\n",
    "\n",
    "env2 = MyCustomEnv(df=fu_df, window_size=12, frame_bound=(12,50))\n",
    "env_maker = lambda: env2\n",
    "env = DummyVecEnv([env_maker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.a2c.policies import MlpPolicy\n",
    "\n",
    "model = A2C(MlpPolicy, env, verbose=1, device='cuda')\n",
    "model.learn(total_timesteps=1000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = MyCustomEnv(df=fu_df, window_size=12, frame_bound=(60,120))\n",
    "state = env.reset()\n",
    "while True:\n",
    "    n_state = n_state[np.newaxis, ...]\n",
    "    action = env.action_space.sample()\n",
    "    n_state, rewards, done, info = env.step(action)\n",
    "    if done:\n",
    "        print('info', info)\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.cla()\n",
    "env.render_all()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
